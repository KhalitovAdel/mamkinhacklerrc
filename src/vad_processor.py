# vad_processor.py
from typing import Literal
import webrtcvad
import struct
import numpy as np
from collections import deque
from scipy import signal

class VADProcessor:
    def __init__(self, sample_rate=16000, channels=1, aggressiveness=3):
        self.vad = webrtcvad.Vad(aggressiveness)
        self.SAMPLE_RATE = sample_rate
        self.CHANNELS = channels
        self.FRAME_DURATION = 30  # ms
        self.CHUNK_SIZE = int(self.SAMPLE_RATE * self.FRAME_DURATION / 1000)
        
        # ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð±ÑƒÑ„ÐµÑ€ (Ð´Ð¾ 60 ÑÐµÐºÑƒÐ½Ð´)
        self.audio_buffer = deque(maxlen=int(60 * 1000 / self.FRAME_DURATION))
        self.is_speaking = False
        self.silence_frames = 0
        self.MIN_SILENCE_FRAMES = 25  # Ð£Ð²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ð¾Ñ€Ð¾Ð³ Ð´Ð»Ñ ÑˆÑƒÐ¼Ð½Ñ‹Ñ… ÑÑ€ÐµÐ´
        self.noise_level = None
        self.speech_counter = 0

    def process_audio_chunk(self, chunk) -> (tuple[Literal['end'], bytes] | Literal['start'] | None):
        """Process audio chunk with noise adaptation"""
        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ Ð² Ð¼Ð¾Ð½Ð¾
        if self.CHANNELS == 2:
            chunk = self.__convert_stereo_to_mono(chunk)
        
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»Ñ ÑˆÑƒÐ¼Ð°
        self.__update_noise_profile(chunk)
        
        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ñ
        processed_chunk = self.__apply_noise_reduction(chunk)
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° VAD Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ñ‹Ð¼ Ð¿Ð¾Ñ€Ð¾Ð³Ð¾Ð¼
        is_speech = self.vad.is_speech(processed_chunk, self.SAMPLE_RATE)
        
        # ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ñ€ÐµÑ‡Ð¸
        if is_speech:
            self.speech_counter += 1
            self.audio_buffer.append(chunk)  # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ chunk
            self.silence_frames = 0
            
            if not self.is_speaking and self.speech_counter > 2:
                print("ðŸ”Š Speech started!")
                self.is_speaking = True
                return "start"
        else:
            if self.is_speaking:
                self.silence_frames += 1
                
                # Ð”Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ñ€Ð¾Ð³ Ñ‚Ð¸ÑˆÐ¸Ð½Ñ‹ Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ ÑƒÑ€Ð¾Ð²Ð½Ñ ÑˆÑƒÐ¼Ð°
                dynamic_threshold = self.MIN_SILENCE_FRAMES
                if self.noise_level and self.noise_level > 2000:  # Ð’Ñ‹ÑÐ¾ÐºÐ¸Ð¹ ÑˆÑƒÐ¼
                    dynamic_threshold += 10
                
                if self.silence_frames >= dynamic_threshold:
                    print(f"ðŸ”‡ Speech ended! (Noise level: {self.noise_level:.1f})")
                    self.is_speaking = False
                    self.speech_counter = 0
                    audio_data = b"".join(self.audio_buffer)
                    self.audio_buffer.clear()

                    data_size = len(audio_data)
                    header = struct.pack(
                        '<4sI4s4sIHHIIHH4sI',
                        b'RIFF',
                        36 + data_size,
                        b'WAVE',
                        b'fmt ',
                        16,  # fmt chunk size
                        1,  # audio format (PCM)
                        self.CHANNELS,
                        self.SAMPLE_RATE,
                        self.SAMPLE_RATE * self.CHANNELS * 2,  # byte rate
                        self.CHANNELS * 2,  # block align
                        16,  # bits per sample
                        b'data',
                        data_size
                    )
                    
                    return "end", header + audio_data
        
        return None

    def __apply_noise_reduction(self, chunk):
        """ÐŸÑ€Ð¾ÑÑ‚Ð¾Ðµ ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ"""
        samples = np.frombuffer(chunk, dtype=np.int16)
        
        # Ð‘Ð°Ð·Ð¾Ð²Ð¾Ðµ ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ (Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ Ð²Ñ‹ÑÐ¾ÐºÐ¸Ñ… Ñ‡Ð°ÑÑ‚Ð¾Ñ‚)
        b, a = signal.butter(5, 300/(self.SAMPLE_RATE/2), 'high')
        filtered = signal.filtfilt(b, a, samples)
        
        return filtered.astype(np.int16).tobytes()

    def __convert_stereo_to_mono(self, data):
        """Convert stereo to mono with noise-aware mixing"""
        samples = struct.unpack(f"<{len(data)//2}h", data)
        mono_data = bytearray()
        
        for i in range(0, len(samples), 2):
            l = samples[i]
            r = samples[i+1] if i+1 < len(samples) else l
            
            # Ð’Ð·Ð²ÐµÑˆÐµÐ½Ð½Ð¾Ðµ ÑÐ¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ ÑƒÑ€Ð¾Ð²Ð½Ñ ÑÐ¸Ð³Ð½Ð°Ð»Ð°
            if abs(l) > abs(r):
                mono_sample = int(l * 0.7 + r * 0.3)
            else:
                mono_sample = int(l * 0.3 + r * 0.7)
                
            mono_data.extend(struct.pack("<h", mono_sample))
        
        return bytes(mono_data)

    def __update_noise_profile(self, chunk):
        """ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð°Ñ ÐºÐ°Ð»Ð¸Ð±Ñ€Ð¾Ð²ÐºÐ° ÑƒÑ€Ð¾Ð²Ð½Ñ ÑˆÑƒÐ¼Ð°"""
        samples = np.frombuffer(chunk, dtype=np.int16)
        current_level = np.mean(np.abs(samples))
        
        if self.noise_level is None:
            self.noise_level = current_level
        else:
            # Ð­ÐºÑÐ¿Ð¾Ð½ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐºÐ¾Ð»ÑŒÐ·ÑÑ‰ÐµÐµ ÑÑ€ÐµÐ´Ð½ÐµÐµ
            self.noise_level = 0.9 * self.noise_level + 0.1 * current_level